{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eca852d0",
   "metadata": {},
   "source": [
    "# Exercise 1: Apache Spark Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b04d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510d88",
   "metadata": {},
   "source": [
    "### 1.Perform rightOuterJoin and fullOuterJoin operations between a and b. Briefly explain your solution.\n",
    "1. used parallelize of feed the data as rdd in spark system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d106e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_a = sc.parallelize([\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b233568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_b = sc.parallelize([\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf3a3ac",
   "metadata": {},
   "source": [
    " 2. mapping the data list in order to reduce later based on the keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c3e2e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark', 1)\n",
      "('rdd', 1)\n",
      "('python', 1)\n",
      "('context', 1)\n",
      "('create', 1)\n",
      "('class', 1)\n"
     ]
    }
   ],
   "source": [
    "a = rdd_a.map(lambda x: (x,1))\n",
    "for element in a.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f28ddaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('operation', 1)\n",
      "('apache', 1)\n",
      "('scala', 1)\n",
      "('lambda', 1)\n",
      "('parallel', 1)\n",
      "('partition', 1)\n"
     ]
    }
   ],
   "source": [
    "b = rdd_b.map(lambda x: (x,1))\n",
    "for element in b.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7ddcd",
   "metadata": {},
   "source": [
    "\n",
    "2. applied Right outer join on the two datasets by using rightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bef64297",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scala\n",
      "operation\n",
      "apache\n",
      "parallel\n",
      "partition\n",
      "lambda\n"
     ]
    }
   ],
   "source": [
    "right_outer_join = a.rightOuterJoin(b)\n",
    "right_outer_join = right_outer_join.map(lambda x : x[0])\n",
    "for element in right_outer_join.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71992c02",
   "metadata": {},
   "source": [
    "3. applied Full outer join on the two datasets by using rightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2056fb38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "context\n",
      "scala\n",
      "rdd\n",
      "operation\n",
      "apache\n",
      "spark\n",
      "create\n",
      "parallel\n",
      "partition\n",
      "lambda\n",
      "class\n"
     ]
    }
   ],
   "source": [
    "full_outer_join = a.fullOuterJoin(b)\n",
    "full_outer_join = full_outer_join.map(lambda x : x[0])\n",
    "for element in full_outer_join.collect():\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee397ba",
   "metadata": {},
   "source": [
    "### Using map and reduce functions to count how many times the character \"s\" appears in all a and b.\n",
    "4. Applying keys to the full outer join output to get the s count from the words in this variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e2d0491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 1),\n",
       " ('context', 1),\n",
       " ('scala', 1),\n",
       " ('rdd', 1),\n",
       " ('operation', 1),\n",
       " ('apache', 1),\n",
       " ('spark', 1),\n",
       " ('create', 1),\n",
       " ('parallel', 1),\n",
       " ('partition', 1),\n",
       " ('lambda', 1),\n",
       " ('class', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_with_s = full_outer_join.map(lambda x: (x,1))\n",
    "a_with_s.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ad374",
   "metadata": {},
   "source": [
    "5. definition intialization for checking the s word. It checks the word at every character ans if the word has a s in it the count variable will be added with 1 each time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d874b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_s(word):\n",
    "    count = 0\n",
    "    for idx in word:\n",
    "        if idx == 's':\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac242b1d",
   "metadata": {},
   "source": [
    "6. Using map with the above definition to assign the number of s in a word as the key to every word in the full outer join output\n",
    "7. The newly mapped keys with the number of s as keys are reduced to get the total number of s in all the words by using spark hadoop map reduce methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "899292a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_s_words = a_with_s.map(lambda x:(x[0], check_for_s(x[0])))\n",
    "count_from_reduce = count_s_words.map(lambda a:a[1]).reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c6de70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_from_reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6667060",
   "metadata": {},
   "source": [
    "### Using aggregate function to count how many times the character \"s\" appears in all a and b.\n",
    "8. now using the aggreagte method of spark we have first done the similar thing as we are first assigning the nuber of s charscters in each word by using the above method and than adding all the counts to the the total number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d77cc376",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_from_aggregate = a_with_s.aggregate(0,lambda a,x: a + check_for_s(x[0]), lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e09ed431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_from_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352a5cf",
   "metadata": {},
   "source": [
    "# Part b) Basic Operations on DataFrames\n",
    "### 1. Replace the null value(s) in column points by the mean of all points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b322fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "df = sqlContext.read.json(\"students.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa6ceb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|  null|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57006391",
   "metadata": {},
   "source": [
    "9. After loading the dataset into sparks dataframe, I have converted the points column into numpy array to calculate mean of all points by looping through the data in array.\n",
    "10. after calculating mean I have replaced the null value in the dataset in points column by using dataframe null fill methods.\n",
    "### Replace the null value(s) in column dob and column last name by \"unknown\" and \"--\" respectively.\n",
    "11. Similarly, filled the null values in the dob and lastname columns with 'unknown' and '--' respectivelty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b0af636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On 11 row is a [None]\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|       --|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       --|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Replace the null value(s) in column points by the mean of all points.\n",
    "import numpy as np\n",
    "import math\n",
    "sum = 0\n",
    "x = np.array(df.select(\"points\").collect()) \n",
    "for i, data in enumerate(x):\n",
    "    if data == None:\n",
    "        print(f'On {i} row is a {data}')\n",
    "    else:\n",
    "        sum = sum + data\n",
    "\n",
    "mean_points = sum/len(x)\n",
    "mean_points[0]\n",
    "df_point = df.na.fill(value=mean_points[0],subset=[\"points\"])\n",
    "df_dob = df_point.na.fill(value='unknown', subset=[\"dob\"])\n",
    "df = df_dob.na.fill(value='--', subset=[\"last_name\"])\n",
    "df.show()\n",
    "#df.na.fill(value='NO VALUE',subset=[\"dob\", \"first_name\", \"last_name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58743c2d",
   "metadata": {},
   "source": [
    "### In the dob column, there exist several formats of dates, e.g. October 14, 1983 and 26 December 1989. Let's convert all the dates into DD-MM-YYYY format where DD, MM and YYYY are two digits for day, two digits for months and four digits for year respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c11b6",
   "metadata": {},
   "source": [
    "1. I wrote a method to assign the month number to the month word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64571154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_string_to_number(string):\n",
    "    m = {\n",
    "        'jan': '01',\n",
    "        'feb': '02',\n",
    "        'mar': '03',\n",
    "        'apr': '04',\n",
    "         'may':'05',\n",
    "         'jun':'06',\n",
    "         'jul':'07',\n",
    "         'aug':'08',\n",
    "         'sep':'09',\n",
    "         'oct':'10',\n",
    "         'nov':'11',\n",
    "         'dec':'12'\n",
    "        }\n",
    "    s = string.strip()[:3].lower()\n",
    "    try:\n",
    "        out = m[s]\n",
    "        return out\n",
    "    except:\n",
    "        raise ValueError('Not a month')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4042db14",
   "metadata": {},
   "source": [
    "#### 2. made another method to convert the date into the format given as DD-MM-YYYY.\n",
    "  ##### a. Firstly, I have intialized the month, date and year variable to split the values from each string and store the       converted parts in these variables\n",
    "  ##### b. Then I have removed commas in the string \n",
    "  ##### c. then I have split the transformed string by space in the string to seperte the words\n",
    "  ##### d. Then the following for loop iterates through the split list of month, date and year.\n",
    "  ##### c. It then checks id the coming string has alphabets which means its going to be the month so it assigns the month its respective month value\n",
    "  ##### d. if the string is not alpha then then it checks the length of the string, if it is four that means year and less than that is month and I then transform the date with a padded 0 if the length is 1 \n",
    "  ##### e. then I am concatinating the transformed strings into the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f35e227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions as F \n",
    "\n",
    "def convert_date(dob):\n",
    "    print(dob)\n",
    "    Month = \"\"\n",
    "    Date = \"\"\n",
    "    Year = \"\"\n",
    "    Date_concat = None\n",
    "    date_string = str(dob)\n",
    "    date_list = []\n",
    "    transformed_string=date_string.replace(\",\",\"\")\n",
    "    print(transformed_string)\n",
    "    list_dm = transformed_string.split(' ')\n",
    "    print(list_dm)\n",
    "    #print(list_dm)\n",
    "    for i , values in enumerate(list_dm):\n",
    "        if values.isalpha():\n",
    "            if values == 'unknown':\n",
    "                Date_concat = 'unknown'\n",
    "                continue\n",
    "            else:\n",
    "                Month = month_string_to_number(values)\n",
    "        else:\n",
    "            if len(values) == 4:\n",
    "                Year = values\n",
    "            else:\n",
    "                Date = values\n",
    "                if len(Date) > 1:\n",
    "                    Date = values\n",
    "                else:\n",
    "                    Date = \"0\"+ Date\n",
    "\n",
    "        Date_concat = Date + \"-\" + Month + \"-\" + Year\n",
    "    return Date_concat\n",
    "    \n",
    "\n",
    "#     df2 = df.withColumn(\"date\", lit(str(lit_lst)))\n",
    "#     print(date_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a32c43b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October 14, 1983\n",
      "October 14 1983\n",
      "['October', '14', '1983']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'14-10-1983'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert a new column age and calculate the current age of all students.\n",
    "#df['new_dob'] = np.array(date_list)\n",
    "value = convert_date('October 14, 1983')\n",
    "value\n",
    "#df2 = df.withColumn(\"date\", lit(str(date_list)))\n",
    "# df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fee463",
   "metadata": {},
   "source": [
    "Adding a new column to the dataframe with the transformed dates by using pyspark sql functions which takes user defined functions to add values by iterating through the dataframe itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27aeef30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "df = df.withColumn(\"new_date\",  F.udf(convert_date, StringType())(F.col(\"dob\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fd97692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "|            course|               dob|first_name|last_name|points|s_id|  new_date|\n",
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|14-10-1983|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|26-09-1980|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|12-06-1982|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|05-04-1987|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|01-11-1978|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|17-02-1981|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|01-01-1984|\n",
      "|     Deep Learning|  January 13, 1978|      John|       --|    10|   8|13-01-1978|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|26-12-1989|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|30-12-1987|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|12-06-1975|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|02-07-1985|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|22-07-1980|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|07-02-1986|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|18-05-1987|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|10-08-1984|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|16-12-1990|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|   unknown|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|07-03-1980|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       --|    10|  20|02-06-1985|\n",
      "+------------------+------------------+----------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c88568c",
   "metadata": {},
   "source": [
    "### Insert a new column age and calculate the current age of all students."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eec4d1",
   "metadata": {},
   "source": [
    "1. for this task, I have made another methed for calulating age of all the students.\n",
    "2. I am using dateutil library to parse the string to datetime formate to calculate the current age of all students\n",
    "3. using datetime to get the current date, which is then subtracted from the date of birth to get teh age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70455f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "from dateutil import parser\n",
    " \n",
    "def age(birthdate):\n",
    "    age = 0\n",
    "    if birthdate == 'unknown':\n",
    "        age = 'unknown'\n",
    "    else:\n",
    "        #date_time_obj = datetime.strptime(birthdate,'%d-%m-%y')\n",
    "        dt_obj = parser.parse(birthdate)\n",
    "        print(dt_obj)\n",
    "        today = date.today()\n",
    "        age = today.year - dt_obj.year - ((today.month, today.day) < (dt_obj.month, dt_obj.day))\n",
    "    return age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dbae151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+----------+-------+\n",
      "|            course|               dob|first_name|last_name|points|s_id|  new_date|    age|\n",
      "+------------------+------------------+----------+---------+------+----+----------+-------+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|14-10-1983|     38|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|26-09-1980|     41|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|12-06-1982|     39|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|05-04-1987|     35|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|01-11-1978|     44|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|17-02-1981|     41|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|01-01-1984|     38|\n",
      "|     Deep Learning|  January 13, 1978|      John|       --|    10|   8|13-01-1978|     44|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|26-12-1989|     32|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|30-12-1987|     34|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|12-06-1975|     46|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|02-07-1985|     37|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|22-07-1980|     41|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|07-02-1986|     36|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|18-05-1987|     35|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|10-08-1984|     37|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|16-12-1990|     31|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|   unknown|unknown|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|07-03-1980|     42|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       --|    10|  20|02-06-1985|     37|\n",
      "+------------------+------------------+----------+---------+------+----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"age\",  F.udf(age, StringType())(F.col(\"new_date\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ca8c0",
   "metadata": {},
   "source": [
    "### Let's consider granting some points for good performed students in the class. For each student, if his point is larger than 1 standard deviation of all points, then we update his current point to 20, which is the maximum.\n",
    "1. Calulating the global standard deviation and mean usind pyspark sql command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a81178e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------+\n",
      "|   stddev(points)|mean(points)|\n",
      "+-----------------+------------+\n",
      "|3.246050231475656|        11.7|\n",
      "+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.createOrReplaceTempView(\"students\")\n",
    "spark.sql(\"select STDDEV(points), mean(points) from students\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbf22c",
   "metadata": {},
   "source": [
    "To caluate the new points, on 1 std formula, I have created the function which calculates the mean and standard deviation plus and minus to get the range.\n",
    "if point is greater than the highest range point than it will be changed to 20 otherwise it remains same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d87b19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_new_score(point):\n",
    "    std = 3.24605\n",
    "    points = 0\n",
    "    mean = 11.7\n",
    "    std_mean_plus = mean+std\n",
    "    std_mean_minus = mean-std\n",
    "    if point > std_mean_plus:\n",
    "        points = 20\n",
    "    else:\n",
    "        points = point\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b249434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"new_points\",  F.udf(assign_new_score, StringType())(F.col(\"points\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "596e2b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+----------+-------+----------+\n",
      "|            course|               dob|first_name|last_name|points|s_id|  new_date|    age|new_points|\n",
      "+------------------+------------------+----------+---------+------+----+----------+-------+----------+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|14-10-1983|     38|        10|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|26-09-1980|     41|        20|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|12-06-1982|     39|        20|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|05-04-1987|     35|        12|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|01-11-1978|     44|        11|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|17-02-1981|     41|        10|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|01-01-1984|     38|        14|\n",
      "|     Deep Learning|  January 13, 1978|      John|       --|    10|   8|13-01-1978|     44|        10|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|26-12-1989|     32|        20|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|30-12-1987|     34|        11|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|12-06-1975|     46|        12|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|02-07-1985|     37|        11|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|22-07-1980|     41|        13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|07-02-1986|     36|        12|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|18-05-1987|     35|         9|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|10-08-1984|     37|         7|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|16-12-1990|     31|         9|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|   unknown|unknown|         6|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|07-03-1980|     42|        20|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       --|    10|  20|02-06-1985|     37|        10|\n",
      "+------------------+------------------+----------+---------+------+----+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb072a22",
   "metadata": {},
   "source": [
    "### Create a histogram on the new points created in the task 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "109e0ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'New Points')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.array(df.select(\"new_points\").collect()) \n",
    "plt.hist(x , color = 'green')\n",
    "plt.xlabel('Data points')\n",
    "plt.ylabel('New Points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6de59",
   "metadata": {},
   "source": [
    "# Exercise 2: Manipulating Recommender Dataset with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151c767f",
   "metadata": {},
   "source": [
    "### A tagging session for a user can be defned as the duration in which he/she generated tagging activities. Typically, an inactive duration of 30 mins is considered as a termination of the tagging session. Your task is to separate out tagging sessions for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "639e38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "df_tags = sqlContext.read.csv(SparkFiles.get(\"C:/Users/HP/tags.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "815c2646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+\n",
      "|UserID|MovieID|                 Tag| Timestamp|\n",
      "+------+-------+--------------------+----------+\n",
      "|    15|   4973|          excellent!|1215184630|\n",
      "|    20|   1747|            politics|1188263867|\n",
      "|    20|   1747|              satire|1188263867|\n",
      "|    20|   2424|     chick flick 212|1188263835|\n",
      "|    20|   2424|               hanks|1188263835|\n",
      "|    20|   2424|                ryan|1188263835|\n",
      "|    20|   2947|              action|1188263755|\n",
      "|    20|   2947|                bond|1188263756|\n",
      "|    20|   3033|               spoof|1188263880|\n",
      "|    20|   3033|           star wars|1188263880|\n",
      "|    20|   7438|              bloody|1188263801|\n",
      "|    20|   7438|             kung fu|1188263801|\n",
      "|    20|   7438|           Tarantino|1188263801|\n",
      "|    21|  55247|                   R|1205081506|\n",
      "|    21|  55253|               NC-17|1205081488|\n",
      "|    25|     50|        Kevin Spacey|1166101426|\n",
      "|    25|   6709|         Johnny Depp|1162147221|\n",
      "|    31|     65|        buddy comedy|1188263759|\n",
      "|    31|    546|strangely compelling|1188263674|\n",
      "|    31|   1091|         catastrophe|1188263741|\n",
      "+------+-------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afa27cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+\n",
      "|UserID|MovieID|                 Tag| Timestamp|\n",
      "+------+-------+--------------------+----------+\n",
      "|    15|   4973|          excellent!|1215184630|\n",
      "|    20|   1747|            politics|1188263867|\n",
      "|    20|   1747|              satire|1188263867|\n",
      "|    20|   2424|     chick flick 212|1188263835|\n",
      "|    20|   2424|               hanks|1188263835|\n",
      "|    20|   2424|                ryan|1188263835|\n",
      "|    20|   2947|              action|1188263755|\n",
      "|    20|   2947|                bond|1188263756|\n",
      "|    20|   3033|               spoof|1188263880|\n",
      "|    20|   3033|           star wars|1188263880|\n",
      "|    20|   7438|              bloody|1188263801|\n",
      "|    20|   7438|             kung fu|1188263801|\n",
      "|    20|   7438|           Tarantino|1188263801|\n",
      "|    21|  55247|                   R|1205081506|\n",
      "|    21|  55253|               NC-17|1205081488|\n",
      "|    25|     50|        Kevin Spacey|1166101426|\n",
      "|    25|   6709|         Johnny Depp|1162147221|\n",
      "|    31|     65|        buddy comedy|1188263759|\n",
      "|    31|    546|strangely compelling|1188263674|\n",
      "|    31|   1091|         catastrophe|1188263741|\n",
      "+------+-------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tags.orderBy(col(\"Timestamp\").desc())\n",
    "df_tags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75ec9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a9089d",
   "metadata": {},
   "source": [
    "### First Step\n",
    "1. getting the previous timestamp for each UserID so that it can be used for calculating the differnce in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "930fbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tags = df_tags.withColumn(\n",
    "    \"session_id\",\n",
    "    F.lag(\"Timestamp\").over(Window.partitionBy(\"UserID\").orderBy(\"Timestamp\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29c1005b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+----------+\n",
      "|UserID|MovieID|                 Tag| Timestamp|session_id|\n",
      "+------+-------+--------------------+----------+----------+\n",
      "|    15|   4973|          excellent!|1215184630|      null|\n",
      "|    20|   2947|              action|1188263755|      null|\n",
      "|    20|   2947|                bond|1188263756|1188263755|\n",
      "|    20|   7438|              bloody|1188263801|1188263756|\n",
      "|    20|   7438|             kung fu|1188263801|1188263801|\n",
      "|    20|   7438|           Tarantino|1188263801|1188263801|\n",
      "|    20|   2424|     chick flick 212|1188263835|1188263801|\n",
      "|    20|   2424|               hanks|1188263835|1188263835|\n",
      "|    20|   2424|                ryan|1188263835|1188263835|\n",
      "|    20|   1747|            politics|1188263867|1188263835|\n",
      "|    20|   1747|              satire|1188263867|1188263867|\n",
      "|    20|   3033|               spoof|1188263880|1188263867|\n",
      "|    20|   3033|           star wars|1188263880|1188263880|\n",
      "|    31|   6373|   comedy of manners|1188263644|      null|\n",
      "|    31|    546|strangely compelling|1188263674|1188263644|\n",
      "|    31|   2116|                Epic|1188263707|1188263674|\n",
      "|    31|   1091|         catastrophe|1188263741|1188263707|\n",
      "|    31|     65|        buddy comedy|1188263759|1188263741|\n",
      "|    48|  54775|The Director Shou...|1215135517|      null|\n",
      "|    48|  54290|Why the terrorist...|1215135611|1215135517|\n",
      "+------+-------+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tags.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439fcf30",
   "metadata": {},
   "source": [
    "### Second Step\n",
    "Assigning 1 to sessions where the differnce of timestamp and previous timestamp is greater than 1800 which is 30 mins otherwise its 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39d92392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+----------+\n",
      "|UserID|MovieID|                 Tag| Timestamp|session_id|\n",
      "+------+-------+--------------------+----------+----------+\n",
      "|    15|   4973|          excellent!|1215184630|         1|\n",
      "|    20|   2947|              action|1188263755|         1|\n",
      "|    20|   2947|                bond|1188263756|         0|\n",
      "|    20|   7438|              bloody|1188263801|         0|\n",
      "|    20|   7438|             kung fu|1188263801|         0|\n",
      "|    20|   7438|           Tarantino|1188263801|         0|\n",
      "|    20|   2424|     chick flick 212|1188263835|         0|\n",
      "|    20|   2424|               hanks|1188263835|         0|\n",
      "|    20|   2424|                ryan|1188263835|         0|\n",
      "|    20|   1747|            politics|1188263867|         0|\n",
      "|    20|   1747|              satire|1188263867|         0|\n",
      "|    20|   3033|               spoof|1188263880|         0|\n",
      "|    20|   3033|           star wars|1188263880|         0|\n",
      "|    31|   6373|   comedy of manners|1188263644|         1|\n",
      "|    31|    546|strangely compelling|1188263674|         0|\n",
      "|    31|   2116|                Epic|1188263707|         0|\n",
      "|    31|   1091|         catastrophe|1188263741|         0|\n",
      "|    31|     65|        buddy comedy|1188263759|         0|\n",
      "|    48|  54775|The Director Shou...|1215135517|         1|\n",
      "|    48|  54290|Why the terrorist...|1215135611|         0|\n",
      "+------+-------+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_tags = df_tags.withColumn(\n",
    "    \"session_id\",\n",
    "    F.when(F.col(\"timestamp\") - F.col(\"session_id\") <= 1800, 0).otherwise(1),\n",
    ")\n",
    "df_tags.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ed5e25",
   "metadata": {},
   "source": [
    "3. creating a unique id per session by calculating sum of all the assigned values above for each user and then using order by of sql to sort in ascendinf order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc1fb8c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+----------+\n",
      "|UserID|MovieID|                 Tag| Timestamp|session_id|\n",
      "+------+-------+--------------------+----------+----------+\n",
      "|    15|   4973|          excellent!|1215184630|         1|\n",
      "|    20|   2947|              action|1188263755|         1|\n",
      "|    20|   2947|                bond|1188263756|         1|\n",
      "|    20|   7438|              bloody|1188263801|         1|\n",
      "|    20|   7438|             kung fu|1188263801|         1|\n",
      "|    20|   7438|           Tarantino|1188263801|         1|\n",
      "|    20|   2424|     chick flick 212|1188263835|         1|\n",
      "|    20|   2424|               hanks|1188263835|         1|\n",
      "|    20|   2424|                ryan|1188263835|         1|\n",
      "|    20|   1747|            politics|1188263867|         1|\n",
      "|    20|   1747|              satire|1188263867|         1|\n",
      "|    20|   3033|               spoof|1188263880|         1|\n",
      "|    20|   3033|           star wars|1188263880|         1|\n",
      "|    31|   6373|   comedy of manners|1188263644|         1|\n",
      "|    31|    546|strangely compelling|1188263674|         1|\n",
      "|    31|   2116|                Epic|1188263707|         1|\n",
      "|    31|   1091|         catastrophe|1188263741|         1|\n",
      "|    31|     65|        buddy comedy|1188263759|         1|\n",
      "|    48|  54775|The Director Shou...|1215135517|         1|\n",
      "|    48|  54290|Why the terrorist...|1215135611|         1|\n",
      "+------+-------+--------------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "df_tags = df_tags.withColumn(\n",
    "    \"session_id\",\n",
    "    F.sum(\"session_id\").over(Window.partitionBy(\"userid\").orderBy(\"timestamp\")),\n",
    ")\n",
    "df_tags.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e813e31",
   "metadata": {},
   "source": [
    "applying dense_rank function of sql which return the rank of each row within a result set partition, with no gaps in the ranking values. The rank of a specific row is one plus the number of distinct rank values that come before that specific row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be98917f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------------+----------+----------+\n",
      "|UserID|MovieID|              Tag| Timestamp|session_id|\n",
      "+------+-------+-----------------+----------+----------+\n",
      "|    15|   4973|       excellent!|1215184630|         1|\n",
      "|    20|   2947|           action|1188263755|         2|\n",
      "|    20|   2947|             bond|1188263756|         2|\n",
      "|    20|   7438|           bloody|1188263801|         2|\n",
      "|    20|   7438|          kung fu|1188263801|         2|\n",
      "|    20|   7438|        Tarantino|1188263801|         2|\n",
      "|    20|   2424|  chick flick 212|1188263835|         2|\n",
      "|    20|   2424|            hanks|1188263835|         2|\n",
      "|    20|   2424|             ryan|1188263835|         2|\n",
      "|    20|   1747|         politics|1188263867|         2|\n",
      "|    20|   1747|           satire|1188263867|         2|\n",
      "|    20|   3033|            spoof|1188263880|         2|\n",
      "|    20|   3033|        star wars|1188263880|         2|\n",
      "|    21|  55247|                R|1205081506|         3|\n",
      "|    21|  55253|            NC-17|1205081488|         3|\n",
      "|    25|   6709|      Johnny Depp|1162147221|         4|\n",
      "|    25|     50|     Kevin Spacey|1166101426|         5|\n",
      "|    31|   1091|      catastrophe|1188263741|         6|\n",
      "|    31|     65|     buddy comedy|1188263759|         6|\n",
      "|    31|   6373|comedy of manners|1188263644|         6|\n",
      "+------+-------+-----------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a unic id per session per user\n",
    "df_tags = df_tags.withColumn(\n",
    "    \"session_id\", F.dense_rank().over(Window.orderBy(\"userid\", \"session_id\"))\n",
    ")\n",
    "df_tags.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a56966",
   "metadata": {},
   "source": [
    "### Once you have all the tagging sessions for each user, calculate the frequency of tagging for each user session.\n",
    "1. here calculating the frequency for each user by using the sql count function and grouping by the session id and users to get the frequecy for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "febdee0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+\n",
      "|UserID|session_id|count|\n",
      "+------+----------+-----+\n",
      "|    15|         1|    1|\n",
      "|    20|         2|   12|\n",
      "|    21|         3|    2|\n",
      "|    25|         4|    1|\n",
      "|    25|         5|    1|\n",
      "|    31|         6|    5|\n",
      "|    32|         7|    1|\n",
      "|    39|         8|    5|\n",
      "|    48|         9|    2|\n",
      "|    49|        10|   15|\n",
      "|    75|        11|    1|\n",
      "|    78|        12|    1|\n",
      "|   109|        13|   11|\n",
      "|   109|        14|    2|\n",
      "|   109|        15|    3|\n",
      "|   109|        16|    1|\n",
      "|   109|        17|    1|\n",
      "|   109|        18|    1|\n",
      "|   109|        19|    4|\n",
      "|   109|        20|    1|\n",
      "+------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parb = df_tags.groupBy(['UserID','session_id']).count()\n",
    "df_parb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213caedb",
   "metadata": {},
   "source": [
    "### Find a mean and standard deviation of the tagging frequency of each user.\n",
    "using sql aggregate function of mean to calculate mean of frequency/count column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f63d864d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n",
      "|UserID|session_id|avg(count)|\n",
      "+------+----------+----------+\n",
      "|    15|         1|       1.0|\n",
      "|    20|         2|      12.0|\n",
      "|    21|         3|       2.0|\n",
      "|    25|         4|       1.0|\n",
      "|    25|         5|       1.0|\n",
      "|    31|         6|       5.0|\n",
      "|    32|         7|       1.0|\n",
      "|    39|         8|       5.0|\n",
      "|    48|         9|       2.0|\n",
      "|    49|        10|      15.0|\n",
      "+------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parb1 = df_parb.groupBy('UserID','session_id').agg({'count': 'mean'})\n",
    "df_parb1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9bc5e",
   "metadata": {},
   "source": [
    "using sql aggregate function of std to calculate std of frequency/count column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe8b8b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|UserID|     stddev(count)|\n",
      "+------+------------------+\n",
      "|    15|              null|\n",
      "|    20|              null|\n",
      "|    21|              null|\n",
      "|    25|               0.0|\n",
      "|    31|              null|\n",
      "|    32|              null|\n",
      "|    39|              null|\n",
      "|    48|              null|\n",
      "|    49|              null|\n",
      "|    75|              null|\n",
      "|    78|              null|\n",
      "|   109|3.2702361450580977|\n",
      "|   127|              null|\n",
      "|   133|              null|\n",
      "|   146| 9.010056652425451|\n",
      "|   147|              null|\n",
      "|   170|              null|\n",
      "|   175|               0.0|\n",
      "|   181|              null|\n",
      "|   190|               7.0|\n",
      "+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parb2 = df_parb.groupBy('UserID').agg({'count': 'stddev'})\n",
    "df_parb2.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099926d2",
   "metadata": {},
   "source": [
    "### Find a mean and standard deviation of the tagging frequency for across users.\n",
    "1. converting the mean into numpy array to calulate the global mean and standard deviation usimg numpy mean function \n",
    "2. for standard deviation calculating the std by using aggregate function od sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "828d222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = np.array(df_parb1.select(\"avg(count)\").collect()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb68f4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  7.299526500687338\n",
      "std:  19.248910998850313\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(avg)\n",
    "print('mean: ',mean)\n",
    "std = df_parb2.agg({'stddev(count)': 'stddev'}).collect()\n",
    "value_std =std[0][0]\n",
    "print('std: ', value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0825aa94",
   "metadata": {},
   "source": [
    "### Provide the list of users with a mean tagging frequency within the two standard deviation from the mean frequency of all users.\n",
    "Calculating std plus by adding the mean and two std and then calculating std minus by subtrating the mean and 2* std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "299f293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_plus = mean+2*std[0][0]\n",
    "std_minus = mean-2*std[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7d67a",
   "metadata": {},
   "source": [
    "here, adding a column by comparing each user count with the std_plus and std_minus. If the count is greater than std_minus than we assign count as 1 otherwise its 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc10d705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---------+-----+\n",
      "|UserID|session_id|Frequency|count|\n",
      "+------+----------+---------+-----+\n",
      "|    15|         1|        1|    1|\n",
      "|    20|         2|       12|    1|\n",
      "|    21|         3|        2|    1|\n",
      "|    25|         4|        1|    1|\n",
      "|    25|         5|        1|    1|\n",
      "|    31|         6|        5|    1|\n",
      "|    32|         7|        1|    1|\n",
      "|    39|         8|        5|    1|\n",
      "|    48|         9|        2|    1|\n",
      "|    49|        10|       15|    1|\n",
      "|    75|        11|        1|    1|\n",
      "|    78|        12|        1|    1|\n",
      "|   109|        13|       11|    1|\n",
      "|   109|        14|        2|    1|\n",
      "|   109|        15|        3|    1|\n",
      "|   109|        16|        1|    1|\n",
      "|   109|        17|        1|    1|\n",
      "|   109|        18|        1|    1|\n",
      "|   109|        19|        4|    1|\n",
      "|   109|        20|        1|    1|\n",
      "|   109|        21|        1|    1|\n",
      "|   127|        22|       26|    1|\n",
      "|   133|        23|        5|    1|\n",
      "|   146|        24|        1|    1|\n",
      "|   146|        25|        1|    1|\n",
      "|   146|        26|        2|    1|\n",
      "|   146|        27|       12|    1|\n",
      "|   146|        28|        2|    1|\n",
      "|   146|        29|        4|    1|\n",
      "|   146|        30|       18|    1|\n",
      "|   146|        31|       53|    0|\n",
      "|   146|        32|        3|    1|\n",
      "|   146|        33|        3|    1|\n",
      "|   146|        34|        2|    1|\n",
      "|   146|        35|        2|    1|\n",
      "|   146|        36|        2|    1|\n",
      "|   146|        37|        1|    1|\n",
      "|   146|        38|        1|    1|\n",
      "|   146|        39|        1|    1|\n",
      "|   146|        40|        4|    1|\n",
      "|   146|        41|        1|    1|\n",
      "|   146|        42|        1|    1|\n",
      "|   146|        43|        2|    1|\n",
      "|   146|        44|        4|    1|\n",
      "|   146|        45|        2|    1|\n",
      "|   146|        46|        2|    1|\n",
      "|   146|        47|        2|    1|\n",
      "|   146|        48|        8|    1|\n",
      "|   146|        49|        4|    1|\n",
      "|   146|        50|        1|    1|\n",
      "+------+----------+---------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_parb = df_parb.withColumnRenamed(\"count\",\"Frequency\")\n",
    "from pyspark.sql.functions import when\n",
    "df_final = df_parb.withColumn(\"count\", when(df_parb.Frequency<std_plus, when(df_parb.Frequency>std_minus,1).otherwise(0)).otherwise(0))\n",
    "df_final.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09618ea4",
   "metadata": {},
   "source": [
    "1. total number of users with in 2 standard deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9796f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|count(count)|\n",
      "+------------+\n",
      "|       12739|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.createOrReplaceTempView(\"data\")\n",
    "spark.sql(\"select count(count) from data where count=1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed62106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
